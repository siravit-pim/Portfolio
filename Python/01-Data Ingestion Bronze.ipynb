{"cells":[{"cell_type":"markdown","source":["# Enrich NYC Yellow Taxi Data with Holiday and Weather\n","\n","In this session, we will be working on enriching NYC Yellow Taxi data by combining it with Holiday and Weather datasets. \n","\n","To get started, we will be using three separate notebooks, following the best practices for Fabric Lakehouse. In the first notebook, we will read the datasets from a Public Azure Storage account and save the data in the BronzeLakehouse. \n","\n","Next, we will be using multiple notebooks to enrich the NYC Yellow Taxi data. In the first notebook, we will provide examples of how to load and clean the NYC Yellow Taxi data.\n","\n"," In the other notebooks, we will be focusing on reading Open Datasets, manipulating the data to prepare for further analysis (including column projection, filtering, grouping, and joins), creating a Spark table for modeling training, and building a Lakehouse table to generate PBI reports.\n","\n","\n"," <font size=\"2\" color=\"red\" face=\"sans-serif\" bold> \n","\n","<b> <i> <u>Make sure that for every Notebook the right Lakehouse is pinned. Basically for this notebook you need to see the BronzeLakehouse under Lakehouses on the left.\n","</font>\n","\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0367f2b2-7632-42df-b607-4f34b98902fb"},{"cell_type":"markdown","source":["\n","Importing the Data types we need for this Notebook"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3a1f6b0c-1470-40b5-b2f2-8bdc37df540e"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType\n","import pyspark.sql.functions as f"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"14acf7cb-cee9-4b57-9826-f9e4911a33d3","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-02T14:34:03.2615123Z","session_start_time":"2023-10-02T14:34:03.4676444Z","execution_start_time":"2023-10-02T14:34:13.8515557Z","execution_finish_time":"2023-10-02T14:34:16.4443734Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"6bb657bb-0e59-49b2-9fd6-88dfa5a8400e"},"text/plain":"StatementMeta(, 14acf7cb-cee9-4b57-9826-f9e4911a33d3, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e96ba356-ced3-49e5-8644-064374db222c"},{"cell_type":"markdown","source":["## Data Ingestion New York Yellow Taxi Data Set\n","We are going to load the New York Yellow Taxi data from a Public Azure Storage Account.\n","\n","<font size=\"2\" color=\"red\" face=\"sans-serif\" bold> \n","\n","<b> <i> <u>No changes are required to this cell, This cell have all the necessary credentials to Ingest data from storage account\n","</font>"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c65bc7c5-830d-42fd-8b79-5296c87131ae"},{"cell_type":"code","source":["# Azure storage access info for open datasets yellow cab\n","storage_account = \"azuresynapsestorage\"\n","container = \"sampledata\"\n","\n","sas_token = r\"\" # Blank since container is Anonymous access\n","\n","# Set Spark config to access  blob storage\n","spark.conf.set(f\"fs.azure.sas.{container}.{storage_account}.blob.core.windows.net\", sas_token)\n","\n","# NOTE TO FELLOW AUTHORS:\n","# We can now easily read all of the parquet files in the directory just by pointing Spark at the directory\n","# But if we need to restrict the years that we read (maybe to match the weather data), then we\n","# can use the \"pathGlobFilter\" parameter.  So line 18 (below) would be something like this:\n","#                .parquet(f\"wasbs://{container}@{storage_account}.blob.core.windows.net/Fabric/NYCTaxiDataset/yellow/\", pathGlobFilter=\"20{18,19,21}.parquet\") \\\n","\n","# Read the parquet files\n","nyc_tlc_df = spark.read \\\n","                .parquet(f\"wasbs://{container}@{storage_account}.blob.core.windows.net/Fabric/NYCTaxiDataset/yellow/\") \\\n","                .cache()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"14acf7cb-cee9-4b57-9826-f9e4911a33d3","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-02T14:34:05.0011021Z","session_start_time":null,"execution_start_time":"2023-10-02T14:34:16.8513207Z","execution_finish_time":"2023-10-02T14:34:22.6425703Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":1,"UNKNOWN":0,"FAILED":0},"jobs":[{"displayName":"parquet at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":8,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 4:\n# Azure storage access info for open datasets yellow cab\nstorage_account = \"azuresynapsestorage\"\ncontainer = \"sampledata\"\n\nsas_token = r\"\" # Blank since container is Anonymous access\n\n# Set Spark config to access  blob storage\nspark.conf.set(f\"fs.azure.sas.{container}.{storage_account}.blob.core.windows.net\", sas_token)\n\n# NOTE TO FELLOW AUTHORS:\n# We can now easily read all of the parquet files in the directory just by pointing Spark at the directory\n# But if we need to restrict the years that we read (maybe to match the weather data), then we\n# can use the \"pathGlobFilter\" parameter.  So line 18 (below) would be something like this:\n#                .parquet(f\"wasbs://{container}@{storage_account}.blob.core.windows.net/Fabric/NYCTaxiDataset/yellow/\", pathGlobFilter=\"20{18,19,21}.parquet\") \n# Read the parquet files\nnyc_tlc_df = spark.read                 .parquet(f\"wasbs://{container}@{storage_account}.blob.core.windows.net/Fabric/NYCTaxiDataset/yellow/\")                 .cache()","submissionTime":"2023-10-02T14:34:18.234GMT","completionTime":"2023-10-02T14:34:19.045GMT","stageIds":[12],"jobGroup":"4","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"88a6870c-7dc9-4b90-9c55-718b7cbe4c1c"},"text/plain":"StatementMeta(, 14acf7cb-cee9-4b57-9826-f9e4911a33d3, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"15ebacef-09da-45a3-aab2-32e26719ffd5"},{"cell_type":"code","source":["# Look at the data that we read\n","display(nyc_tlc_df)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":false},"id":"4ebfefe7-55b3-45c1-b8f3-1ffe2421e313"},{"cell_type":"markdown","source":["Now as we read the data from the external Storage account, we can save the files on the bronze layer. Hence whenever we need to come back to the original files, we can simply read the raw parquet file from Bronze Layer.\n","\n","<font size=\"2\" color=\"red\" face=\"sans-serif\" bold> \n","\n","<b> <i> <u>Make sure that the BronzeLakehouse is pinned for this Notebook.   \n","</font>"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"34f8f237-ada2-4922-8763-86eb0f11d756"},{"cell_type":"code","source":["# Save the data Frame renamedcolumns_df_expand as raw files in the Bronze Layer\n","# You can also use the ABFSS path from BronzeLake Files #Sample ABFSS Path \"abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db/Files/NYCTaxiRawFiles\"\n","# Since we have the BronzeLakehouse pinned for this notebook we just use Files/NYCTaxiRawFiles\n","\n","nyc_tlc_df.write.mode(\"overwrite\").parquet(\"Files/NYCTaxiRawFiles\")\n","print (\"Raw files saved on the Bronze layer\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"14acf7cb-cee9-4b57-9826-f9e4911a33d3","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-02T14:34:53.6195562Z","session_start_time":null,"execution_start_time":"2023-10-02T14:34:53.9891864Z","execution_finish_time":"2023-10-02T14:34:54.4163779Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"80065259-25ef-46e5-9b4d-45c3d380a07b"},"text/plain":"StatementMeta(, 14acf7cb-cee9-4b57-9826-f9e4911a33d3, 6, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Raw files saved on the Bronze layer\n"]}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f3585752-96fd-4cfb-9540-262220aa8e94"},{"cell_type":"markdown","source":["As the data has been loaded into the Data Frame, let's explore the data.\n","The `nyc_tlc_df` dataframe contains several years of data.  Let's see which years we have and how many taxi trips were recorded in each of those years."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"47f0de72-4b96-47e9-acbc-6947255da15d"},{"cell_type":"code","source":["nyc_tlc_df.groupBy(f.year(\"tpep_pickup_datetime\").alias(\"year\")).count().orderBy(\"year\").show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a91da8b3-1373-461e-ad15-26e0b1f9853b"},{"cell_type":"markdown","source":["Now that the initial data is loaded. Let's do some projection on the data to \n","- create new columns for the month number, day of month, day of week, and hour of day. These info is going to be used in the training model to factor in time-based seasonality.\n","- add a static feature for the country code to join holiday data. "],"metadata":{},"id":"c1224a68-e0a2-4daf-b772-04fcfecd8c64"},{"cell_type":"code","source":["# renaming the columns from the raw big data set to better run the queries(in the end the big data set will look like the one that can be imported from Azure Open DataSets)\n","renamedcolumns_df = nyc_tlc_df\\\n","                    .withColumnRenamed(\"VendorID\", \"vendorID\")\\\n","                    .withColumnRenamed(\"tpep_Pickup_DateTime\", \"tpepPickupDateTime\")\\\n","                    .withColumnRenamed(\"tpep_dropoff_datetime\", \"tpepDropoffDatetime\")\\\n","                    .withColumnRenamed(\"passenger_count\", \"passengerCount\")\\\n","                    .withColumnRenamed(\"RatecodeID\", \"ratecodeID\")\\\n","                    .withColumnRenamed(\"trip_distance\", \"tripDistance\")\\\n","                    .withColumnRenamed(\"store_and_fwd_flag\", \"storeAndFwdFlag\")\\\n","                    .withColumnRenamed(\"PULocationID\", \"pULocationID\")\\\n","                    .withColumnRenamed(\"DOLocationID\", \"doLocationID\")\\\n","                    .withColumnRenamed(\"payment_type\", \"paymentType\")\\\n","                    .withColumnRenamed(\"fare_amount\", \"fareAmount\")\\\n","                    .withColumnRenamed(\"extra\", \"extra\")\\\n","                    .withColumnRenamed(\"mta_tax\", \"mtaTax\")\\\n","                    .withColumnRenamed(\"tip_amount\", \"tipAmount\")\\\n","                    .withColumnRenamed(\"tolls_amount\", \"tollsAmount\")\\\n","                    .withColumnRenamed(\"improvement_surcharge\", \"improvementSurcharge\")\\\n","                    .withColumnRenamed(\"total_amount\", \"totalAmount\")\\\n","                    .withColumnRenamed(\"congestion_surcharge\", \"congestionSurcharge\")\\\n","                    .withColumnRenamed(\"airport_fee\", \"airportFee\")\n","\n","display(renamedcolumns_df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"bd2e3065-c09f-4607-91d2-5b6665a9803e"},{"cell_type":"markdown","source":["After the data is read, we'll want to do some initial filtering to clean the dataset. Hence we need to extract the year, month, day of the month and day of the week that will be further used in the joins with Holiday and Weather datasets.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"784b503e-a499-4ee0-831a-bd32771c7969"},{"cell_type":"code","source":["# Extract month, day of month, and day of week from pickup datetime and add a static column for the country code to join holiday data. \n","renamedcolumns_df_expand = renamedcolumns_df.withColumn('datetime',f.to_timestamp('tpepPickupDateTime'))\\\n","                            .withColumn('year',f.year(renamedcolumns_df.tpepPickupDateTime))\\\n","                            .withColumn('month_num',f.month(renamedcolumns_df.tpepPickupDateTime))\\\n","                            .withColumn('day_of_month',f.dayofmonth(renamedcolumns_df.tpepPickupDateTime))\\\n","                            .withColumn('day_of_week',f.dayofweek(renamedcolumns_df.tpepPickupDateTime))\\\n","                            .withColumn('hour_of_day',f.hour(renamedcolumns_df.tpepPickupDateTime))\\\n","                            .withColumn('country_code',f.lit('US'))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"16415818-681f-4094-9353-9cfc64909408","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-09-25T13:45:25.267178Z","session_start_time":null,"execution_start_time":"2023-09-25T13:45:25.6144092Z","execution_finish_time":"2023-09-25T13:45:26.9725003Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"UNKNOWN":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"23ae3aef-b878-4ba7-a449-5127e13ef042"},"text/plain":"StatementMeta(, 16415818-681f-4094-9353-9cfc64909408, 8, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{},"id":"10ff2b35-b705-4de5-9efc-e2365edc8035"},{"cell_type":"markdown","source":[" Storing the DataFrame \n","Now with its columns renamed, and the data typle columns extraction into a table within the Bronze Layer. Going forward, there's no need to retrieve the data from the Storage account and extract the dates and execute the column renaming operations. Instead, we can just generate a new data frame by directly accessing the Bronze Layer.\n","\n","<font size=\"2\" color=\"red\" face=\"sans-serif\" bold> \n","\n","<b> <i> <u>Make sure that the BronzeLakehouse is pinned for this Notebook.   \n","</font>"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"99183e28-4211-4bf7-9037-6346284d3a98"},{"cell_type":"code","source":["# Save the data Frame renamedcolumns_df_expand as a table in the Bronze Layer\n","# Example for the last line in this cell: save(\"abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db\" + table_name)\n","# Use the ABFSS path from BronzeLake Tables  \n","\n","from pyspark.sql.functions import col, year, month, quarter\n","\n","table_name = 'NYCTaxi_Raw'\n","\n","\n","\n","renamedcolumns_df_expand \\\n","    .write \\\n","    .mode(\"overwrite\") \\\n","    .format(\"delta\") \\\n","    .save(\"Tables/\" + table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"16415818-681f-4094-9353-9cfc64909408","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2023-09-25T13:45:31.2979279Z","session_start_time":null,"execution_start_time":"2023-09-25T13:45:31.6488502Z","execution_finish_time":"2023-09-25T13:51:28.5980855Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"UNKNOWN":0,"SUCCEEDED":6},"jobs":[{"displayName":"toString at String.java:2994","dataWritten":0,"dataRead":4853,"rowCount":50,"usageDescription":"","jobId":20,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\n#Save the data Frame renamedcolumns_df_expand as a table in the Bronze Layer\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'NYCTaxiRaw'\n\n\n\nrenamedcolumns_df_expand     .write     .mode(\"overwrite\")     .format(\"delta\")     .save(\"abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db/Tables/\" + table_name): Compute snapshot for version: 0","submissionTime":"2023-09-25T13:51:27.611GMT","completionTime":"2023-09-25T13:51:27.683GMT","stageIds":[27,28,26],"jobGroup":"9","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":4853,"dataRead":16677,"rowCount":67,"usageDescription":"","jobId":19,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\n#Save the data Frame renamedcolumns_df_expand as a table in the Bronze Layer\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'NYCTaxiRaw'\n\n\n\nrenamedcolumns_df_expand     .write     .mode(\"overwrite\")     .format(\"delta\")     .save(\"abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db/Tables/\" + table_name): Compute snapshot for version: 0","submissionTime":"2023-09-25T13:51:26.066GMT","completionTime":"2023-09-25T13:51:27.573GMT","stageIds":[24,25],"jobGroup":"9","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"toString at String.java:2994","dataWritten":16677,"dataRead":32485,"rowCount":34,"usageDescription":"","jobId":18,"name":"toString at String.java:2994","description":"Delta: Job group for statement 9:\n#Save the data Frame renamedcolumns_df_expand as a table in the Bronze Layer\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'NYCTaxiRaw'\n\n\n\nrenamedcolumns_df_expand     .write     .mode(\"overwrite\")     .format(\"delta\")     .save(\"abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db/Tables/\" + table_name): Compute snapshot for version: 0","submissionTime":"2023-09-25T13:51:24.111GMT","completionTime":"2023-09-25T13:51:24.737GMT","stageIds":[23],"jobGroup":"9","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"Job group for statement 9:\n#Save the data Frame renamedcolumns_df_expand as a table in the Bronze Layer\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'NYCTaxiRaw'\n\n\n\nrenamedcolumns_df_expand     .write     .mode(\"overwrite\")     .format(\"delta\")     .save(\"abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db/Tables/\" + table_name)","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":17,"name":"","description":"Job group for statement 9:\n#Save the data Frame renamedcolumns_df_expand as a table in the Bronze Layer\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'NYCTaxiRaw'\n\n\n\nrenamedcolumns_df_expand     .write     .mode(\"overwrite\")     .format(\"delta\")     .save(\"abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db/Tables/\" + table_name)","submissionTime":"2023-09-25T13:51:22.327GMT","completionTime":"2023-09-25T13:51:22.327GMT","stageIds":[],"jobGroup":"9","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":9120741235,"dataRead":16301880874,"rowCount":792354108,"usageDescription":"","jobId":16,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\n#Save the data Frame renamedcolumns_df_expand as a table in the Bronze Layer\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'NYCTaxiRaw'\n\n\n\nrenamedcolumns_df_expand     .write     .mode(\"overwrite\")     .format(\"delta\")     .save(\"abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db/Tables/\" + table_name)","submissionTime":"2023-09-25T13:48:19.974GMT","completionTime":"2023-09-25T13:51:22.010GMT","stageIds":[21,22],"jobGroup":"9","status":"SUCCEEDED","numTasks":57,"numActiveTasks":0,"numCompletedTasks":14,"numSkippedTasks":43,"numFailedTasks":8,"numKilledTasks":0,"numCompletedIndices":14,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"displayName":"save at NativeMethodAccessorImpl.java:0","dataWritten":16301880874,"dataRead":17668682486,"rowCount":641248663,"usageDescription":"","jobId":15,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 9:\n#Save the data Frame renamedcolumns_df_expand as a table in the Bronze Layer\nfrom pyspark.sql.functions import col, year, month, quarter\n\ntable_name = 'NYCTaxiRaw'\n\n\n\nrenamedcolumns_df_expand     .write     .mode(\"overwrite\")     .format(\"delta\")     .save(\"abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db/Tables/\" + table_name)","submissionTime":"2023-09-25T13:45:35.090GMT","completionTime":"2023-09-25T13:48:19.914GMT","stageIds":[20],"jobGroup":"9","status":"SUCCEEDED","numTasks":43,"numActiveTasks":0,"numCompletedTasks":43,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":43,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"2e49444c-3024-4d8d-82fe-c5af00185de6"},"text/plain":"StatementMeta(, 16415818-681f-4094-9353-9cfc64909408, 9, Finished, Available)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"c0f5983d-d8f2-4f1d-8ab5-1883f62cd4df\",\"activityId\":\"16415818-681f-4094-9353-9cfc64909408\",\"applicationId\":\"application_1695647429857_0001\",\"jobGroupId\":\"9\",\"advices\":{\"warn\":2,\"error\":1}}"}},"id":"095ec731-a02e-49bf-aafb-6e31cc977441"},{"cell_type":"markdown","source":["#### Reading the Data from Files vs Reading the Data from Table\n","\n","On the next 2 Cells we will see the difference to read the data from files vs Tables. \n","Reading from files is much faster than reading from tables.\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bee9a0da-30d9-4012-ac6c-f3f17bf523a6"},{"cell_type":"code","source":["# you can also run the bellow command as follow: renamedcolumns_df_expand = spark.read.parquet(\"abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db/Files/NYCTaxiRawFiles\")\n","# The abfss path will be used when you read the data from a different lakehouse than is pinned to the current Notebook.\n","renamedcolumns_df_expand = spark.read.parquet(\"Files/NYCTaxiRawFiles\")\n","display(renamedcolumns_df_expand)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"1a4a82ca-d9b6-4c25-b2d5-3ad3b19f5b55"},{"cell_type":"markdown","source":["Reading the data from the table. Make sure that you replace with your BronzeLakehouse "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2b3aa714-cbb8-4424-bf03-e54fab160107"},{"cell_type":"code","source":["# You will be able to read data from different tables from different lakehouses from the same workspace by creating a dataframe\n","# df = spark.read.table(\"LakehouseName.Tablename\")\n","# In this example \n","renamedcolumns_df_expand = spark.read.table(\"BronzeLakehouse.NYCTaxi_Raw\")\n","display(renamedcolumns_df_expand)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"42a1c50b-4953-448d-9c36-ca9fde3480db"},{"cell_type":"markdown","source":["We might remove unneeded columns from the dataframe. This will reduce the overall size of the dataset and in the end we will have less columns to deal with"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"715c044f-bd01-4f2b-a85f-7b3068d2624f"},{"cell_type":"code","source":["# Remove unused columns from nyc yellow data\n","\n","columns_to_remove = [\n","                     \"lpepDropoffDatetime\", \"puLocationId\", \"doLocationId\", \"pickupLongitude\", \n","                     \"pickupLatitude\", \"dropoffLongitude\",\"dropoffLatitude\" ,\"rateCodeID\", \n","                     \"storeAndFwdFlag\",\"paymentType\", \"fareAmount\", \"extra\", \"mtaTax\",\n","                     \"improvementSurcharge\", \"tollsAmount\", \"ehailFee\", \"tripType\"  \n","                    ]\n","\n","nyc_tlc_df_clean = renamedcolumns_df_expand.drop(*columns_to_remove)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-09-20T13:03:23.4101074Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"a36a3263-1316-48d8-bf9b-7d87eb9147b4"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":22,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"be8cb679-df09-4903-9eeb-231556d611c2"},{"cell_type":"markdown","source":["Let's display the new dataframe to check on the columns we have selected"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"375d130a-c957-4ac1-ac11-5a63f885c90c"},{"cell_type":"code","source":["display(nyc_tlc_df_clean)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"49df36a9-f4b8-4458-a5d5-0e6cd54a9b6f"},{"cell_type":"markdown","source":["By printing the schema of the DataFrame, we gain a deeper understanding of the data types and the specific columns that have been selected. This allows us to more effectively analyze and manipulate the data to extract meaningful insights."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ca40c712-6d50-4d79-908c-e9a82e5469a8"},{"cell_type":"code","source":["nyc_tlc_df_clean.printSchema()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-09-20T13:06:09.9775455Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"71508bf7-fc99-4848-8b7e-9ed7832b1f34"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- vendorID: long (nullable = true)\n |-- tpepPickupDateTime: timestamp (nullable = true)\n |-- tpepDropoffDatetime: timestamp (nullable = true)\n |-- passengerCount: long (nullable = true)\n |-- tripDistance: double (nullable = true)\n |-- tipAmount: double (nullable = true)\n |-- totalAmount: double (nullable = true)\n |-- congestionSurcharge: double (nullable = true)\n |-- airportFee: double (nullable = true)\n |-- source_file: string (nullable = true)\n |-- datetime: timestamp (nullable = true)\n |-- year: integer (nullable = true)\n |-- month_num: integer (nullable = true)\n |-- day_of_month: integer (nullable = true)\n |-- day_of_week: integer (nullable = true)\n |-- hour_of_day: integer (nullable = true)\n |-- country_code: string (nullable = false)\n\n"]}],"execution_count":33,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5d4bf3fb-8ef9-4d37-9ab5-d72ccf566009"},{"cell_type":"markdown","source":["We are exploring further the dataframe by filtering it by the year"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d805c929-bc02-4ed4-b3ab-85e0252b08df"},{"cell_type":"code","source":["display(nyc_tlc_df_clean.filter(\"year == 2019\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"67a22c53-0c8d-4abc-b99a-243ace5516f4"},{"cell_type":"markdown","source":["We can filter one more time to check against what years we have loaded the dataframe"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7d861bcc-4af5-40c6-ae2e-972e2a1d7e3a"},{"cell_type":"code","source":["display(nyc_tlc_df_clean.select(\"year\").distinct().orderBy(\"year\"))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-09-20T11:46:52.7988044Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"971c5437-0be8-466a-b490-e64e526f8ade"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":27,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"705ca438-a5ba-4acf-9407-9b39020b30ae"},{"cell_type":"markdown","source":["## Writing a Delta Table\n","To begin, we have to check the existing tables in the SilverLakehouse. Once this is done, we should check whether the desired table has been successfully created. If the table exists, we can proceed by uncommenting the command provided below and executing it"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e8c8df30-cf3e-4d80-b9b7-9d0c33373bde"},{"cell_type":"code","source":["%%sql\n","--DROP TABLE IF EXISTS Silverlakehouse.NYCTaxi_Clean\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-09-20T11:46:52.9977532Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"e2c72535-c60f-40cb-b434-e46d4241353e"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":16,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"29b29867-902b-484a-8f44-4e286e5537e3"},{"cell_type":"markdown","source":["As the final step in our data processing pipeline, we can save the cleaned and filtered DataFrame, `nyc_tlc_df_clean`, as a table in the Silver layer. However, since we are saving the table in a different Lakehouse, namely Silver, we need to provide the ABFSS path. \n","\n","<font size=\"2\" color=\"red\" face=\"sans-serif\"> \n","Replace what is inside of <> with the abfss path from your SilverLakehouse  \n","</font>\n","\n","Please note that the ABFSS path will be unique to your specific environment and should look similar to the following:\n","\n","`abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/a6114b59-17f1-45b7-a5f1-9d4fd93a92d8/Tables/`\n","\n","To retrieve the ABFSS path, please follow these steps:\n"," 1. Go to the Workspace and click on the SilverLakehouse.\n"," 1. Navigate to the Tables section and select the desired table.\n"," 1. Click on Properties and copy the ABFSS path provided.\n","\n","By following these steps, you can ensure that your DataFrame is saved in the appropriate location and can be easily accessed for any future data transformations."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"63f7e821-fab8-40f2-b92f-cfdb4a48e841"},{"cell_type":"code","source":["# Save the DF clean file as a table in the SilverLakehouse\n","# Example of abfss path:.save(\"abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/a6114b59-17f1-45b7-a5f1-9d4fd93a92d8/Tables/\" + table_name)\n","from pyspark.sql.functions import col, year, month, quarter\n","\n","table_name = 'NYCTaxi_Clean'\n","\n","#df = spark.read.format(\"parquet\").load('Files/green_tripdata_2018-07.parquet')\n","#df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)\n","\n","nyc_tlc_df_clean \\\n","    .write \\\n","    .mode(\"overwrite\") \\\n","    .format(\"delta\") \\\n","    .save(\"<Replace with your SilverLakehouse abfss path>/Tables/\" + table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-09-20T11:46:53.3293037Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"2a86f132-a039-4871-b68f-20501f929262"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":21,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4f3184df-7aa8-4a5b-aab9-e71d89d80b74"},{"cell_type":"markdown","source":["Now we just read the table we created in the Silverlakehouse by creating a dataframe and run the display command"],"metadata":{},"id":"dc7628c9-39fd-464e-b605-458a07f1c883"},{"cell_type":"code","source":["df = spark.read.table(\"Silverlakehouse.NYCTaxi_Clean\")\n","display(df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-09-20T11:46:53.5456033Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"01e32f6a-7a78-4f03-9309-5cb3e7dc17a9"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"c65e89e0-23ad-41f3-a574-6e452f1d8a86"},{"cell_type":"markdown","source":["##  Data Ingestion Holiday data set\n","We are going to load the Holiday  data from a Public Azure Storage Account. This is the second dataset that we are using for this lab and it will be joined with the New York Yellow Taxi data.\n","\n","<font size=\"2\" color=\"red\" face=\"sans-serif\" bold> \n","\n","<b> <i> <u>No changes are Required to this Cell,This Cell have all the Necessary Credentials to Ingest Data from Storage Account\n","</font>\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"abdbbd2b-8b8d-4ab4-8a67-c955fe2f360b"},{"cell_type":"code","source":["# Azure storage access info for open datasets yellow cab\n","storage_account = \"azuresynapsestorage\"\n","container = \"sampledata\"\n","\n","sas_token = r\"\" # Blank since container is Anonymous access\n","\n","# Set Spark config to access  blob storage\n","spark.conf.set(\"fs.azure.sas.%s.%s.blob.core.windows.net\" % (container, storage_account),sas_token)\n","\n","wasbs_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/Fabric/Holiday/\"\n","\n","hol_df = spark.read.format(\"csv\").option(\"header\",\"true\").load(wasbs_path)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4b9cc3c4-20cc-41a5-b415-d4ccffbe708d"},{"cell_type":"markdown","source":["Now as we read the data from the external Storage account, we can save the files on the bronze layer. Hence whenever we need to come back to the original files, we can simply read the raw parquet file from Bronze Layer"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"05ac8a47-5a0c-41af-990d-2fb17d11eb6e"},{"cell_type":"code","source":["# Save the data Frame hol_df as raw file in the Bronze Layer\n","# Example of abfss path:abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db\n","\n","hol_df.write.mode(\"overwrite\").parquet(\"Files/HolidayRawFiles\")\n","print(\"Holiday Raw files saved on the Bronze layer\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-09-28T13:59:17.9041377Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"ad2b537c-1a9e-4e12-9430-12c273d7c84b"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Holiday Raw files saved on the Bronze layer\n"]}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"673cc102-0d9c-4e4e-857e-b5163804bdb7"},{"cell_type":"markdown","source":["<font size=\"2\" color=\"red\" face=\"sans-serif\" bold> \n","\n","<b> <i> <u> Since we still have the BronzeLakehouse attached to this Notebook, we can read the files without specifying the abfss path\n","</font>\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b0fb97f4-598d-42f6-b8b9-15290327abf2"},{"cell_type":"code","source":["# Here we are going to read the data directly from the bronze layer\n","# you can use the abfss path as well to poin to the exact location of the files: abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db/\n","\n","# Let's display the hol_df data frame from the Bronze files \n","\n","hol_df = spark.read.format(\"parquet\").load(\"Files/HolidayRawFiles/\")\n","display(hol_df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"0fec8fcb-ab77-411f-9df0-ecbe31bf74d6"},{"cell_type":"markdown","source":["##  Data Ingestion Weather data set\n","We are going to load the Weather data from a Public Azure Storage Account. This is the third and the last dataset that we are using for this lab and it will be joined with the New York Yellow Taxi data and Holiday dataset.\n","\n","<font size=\"4\" color=\"red\" face=\"sans-serif\"> \n","<font size=\"2\" color=\"red\" face=\"sans-serif\" bold> \n","\n","<b> <i> <u>\n","This cell have all the necessary credentials to Ingest data from storage account\n","No changes are required to this cell  \n","</font>"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"836488ee-f31c-4128-b91c-5a437bdca72b"},{"cell_type":"code","source":["# Reading the data from the Azure Public Synapse Storage Account\n","storage_account = \"azuresynapsestorage\"\n","container = \"sampledata\"\n","\n","sas_token = r\"\" # Blank since container is Anonymous access\n","\n","# Set Spark config to access  blob storage\n","spark.conf.set(\"fs.azure.sas.%s.%s.blob.core.windows.net\" % (container, storage_account),sas_token)\n","\n","isd_df  = spark.read.parquet(f\"wasbs://{container}@{storage_account}.blob.core.windows.net/Fabric/Weather\") \\\n","                .withColumn(\"source_file\", f.input_file_name()) \\\n","                .cache()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-09-28T14:45:23.6325186Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"b63e5740-59f9-4b54-82cf-150f8d9a253b"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"80e0b831-76d2-48cd-a19f-523d8c0da9d6"},{"cell_type":"code","source":["# Save the Data Frame isd_df as raw file in the BronzeLakehouse\n","\n","isd_df.write.mode(\"overwrite\").parquet(\"Files/WeatherRawFiles\")\n","print(\"Weather Raw files saved on the Bronze layer\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-09-28T14:45:43.4200739Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"b596881e-f4c5-42fa-b872-00475624a139"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Weather Raw files saved on the Bronze layer\n"]}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0247fd96-6ade-40e3-9ce7-d3fbff4b768d"}],"metadata":{"microsoft":{"host":{"synapse_widget":{"token":"cf779eda-53c5-46e9-aa7d-ad8553d8088e","state":{}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}