{"cells":[{"cell_type":"markdown","source":["## Enrich the Previously Ingested Dataset with Holidays Dataset\n","Now that we have taxi data downloaded prepared and saved on the Silver Layer as a table, let's add in holiday data as additional features. Holiday-specific features will assist model accuracy, as major holidays are times where taxi demand increases dramatically and supply becomes limited. \n","\n","Let's load the [public holidays]() from the Bronze Layer.\n","\n","\n","<font size=\"2\" color=\"red\" face=\"sans-serif\" bold> \n","\n","<b> <i> <u>Make sure that you pin this Notebook to the SilverLakehouse. To do this, click on Lakehouses, and make sure that the SilverLakehouse is the one that has the pinned sign for this Notebook.\n","</font>\n","\n","\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"95e38b88-a571-4a6c-ae26-495f85e3cfe3"},{"cell_type":"markdown","source":["\n","Import the Data types we need for this Notebook"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3da61aae-5431-49ac-934c-c22746d0456b"},{"cell_type":"code","source":["from datetime import datetime\n","import pyspark.sql.functions as f"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"a7ee5eb5-3d8f-4bb1-bc4a-799dee887287","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2023-09-28T14:05:38.4151328Z","session_start_time":null,"execution_start_time":"2023-09-28T14:05:38.7785688Z","execution_finish_time":"2023-09-28T14:05:47.7220903Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"UNKNOWN":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"d16b14e3-2609-46e8-bae1-5113a073389f"},"text/plain":"StatementMeta(, a7ee5eb5-3d8f-4bb1-bc4a-799dee887287, 13, Finished, Available)"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"06f43eb1-9d05-460c-aa28-62f8a719ded2"},{"cell_type":"markdown","source":["##  Data Ingestion  -- Holiday Data et\n","We are going to load the Holiday  data from the bronze layer\n","\n","<font size=\"2\" color=\"red\" face=\"sans-serif\" bold> \n","\n","<b> <i> <u>We are going to Load the Holiday Data from the Bronze Layer, Which means that you need to Retrieve the ABFSS path from the Bronze Layer.\n","This is required because the SilverLakehouse is pinned to this Notebook\n","</font>"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1ecdc97c-f3de-42a2-8462-70b1a805c7a5"},{"cell_type":"code","source":["# Let's display the hol_df data frame from the Bronze files \n","# Sample ABFSS Path: abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db\n","\n","hol_df = spark.read.format(\"parquet\").load(\"<Replace with your BronzeLakehouse ABFSS path>/Files/HolidayRawFiles/\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"a7ee5eb5-3d8f-4bb1-bc4a-799dee887287","statement_id":10,"state":"finished","livy_statement_state":"available","queued_time":"2023-09-28T14:05:23.126146Z","session_start_time":null,"execution_start_time":"2023-09-28T14:05:23.586798Z","execution_finish_time":"2023-09-28T14:05:24.7238119Z","spark_jobs":{"numbers":{"RUNNING":0,"FAILED":0,"UNKNOWN":0,"SUCCEEDED":1},"jobs":[{"displayName":"load at NativeMethodAccessorImpl.java:0","dataWritten":0,"dataRead":0,"rowCount":0,"usageDescription":"","jobId":20,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 10:\n# Let's display the hol_df data frame from the Bronze files \n\nhol_df = spark.read.format(\"parquet\").option(\"header\",\"true\").load(\"abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/22c420ed-63ea-4c95-9d01-302573d1d5db/Files/HolidayRawFiles/part-00000-8937954e-dac1-4d8b-9648-5124a27b8382-c000.snappy.parquet\")","submissionTime":"2023-09-28T14:05:24.064GMT","completionTime":"2023-09-28T14:05:24.104GMT","stageIds":[24],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"db0668fd-c58e-4397-928a-c849421da677"},"text/plain":"StatementMeta(, a7ee5eb5-3d8f-4bb1-bc4a-799dee887287, 10, Finished, Available)"},"metadata":{}}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"8a710b73-c710-4f62-add5-6f091c71554b"},{"cell_type":"markdown","source":["Display the Data Frame for the Holiday data. We are going to limit the result to 10 rows"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6b417b5e-5bd2-44a7-82d8-f3e87e9a0859"},{"cell_type":"code","source":["# hol_df now is a Spark DataFrame containing CSV data from  the above path \n","display(hol_df.head(10))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"7fbbcf0e-b595-46f9-b55d-5f73e8c0aecb"},{"cell_type":"markdown","source":["Rename the countryRegionCode and date columns to match the respective field names from the taxi data, and normalize the time so it can be used as a key."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"433b61a9-8ebc-4d16-b4fa-ffa9ac8ff0c4"},{"cell_type":"code","source":["hol_df_clean = hol_df \\\n","                .withColumnRenamed('countryRegionCode','country_code') \\\n","                .withColumn('datetime',f.to_date('date'))\n","\n","hol_df_clean.show(5)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b09a908e-6c72-4eac-a715-7296638cbd7e"},{"cell_type":"markdown","source":["## Writing a Delta Table\n","Before proceeding, it's a good idea to check if the table already exists. If it doesn't, you'll need to un-comment the command below and execute the cell to create it."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a930f0c4-ffe6-478f-a257-b0f32664c879"},{"cell_type":"code","source":["%%sql\n","--DROP TABLE IF EXISTS Holiday_Clean"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"}},"id":"9ec4618b-f0b1-4c7a-9cf3-c1bb28e7d461"},{"cell_type":"markdown","source":["## Save the Clean File as a Table in the Silver Layer by Providing the ABFSS Path\n","\n","To retrieve the ABFS path, please follow these steps:\n","\n","1. Go to the Workspace and click on the Silver Lakehouse.\n","1. Navigate to the Tables section and select the desired table.\n","1. Click on Properties and copy the ABFS path provided."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f7d5e901-4756-4a24-9747-ad7f172de677"},{"cell_type":"code","source":["# Save the clean file as a table in the Silver Lakehouse\n","# Sample ABFSS Path: abfss://d239837d-5508-4a0c-acf9-8699feb71c5a@msit-onelake.dfs.fabric.microsoft.com/a6114b59-17f1-45b7-a5f1-9d4fd93a92d8\n","table_name = 'Holiday_Clean'\n","\n","hol_df_clean.write \\\n","    .mode(\"overwrite\") \\\n","    .format(\"delta\") \\\n","    .save(\"Tables/\" + table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-09-20T12:17:50.0777583Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"f6ccc40d-4a87-4eb1-b755-1f7af95d6174"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"20768cd4-1ff3-43bb-ac6f-91e232116ea0"},{"cell_type":"markdown","source":["Since we saved the `NYCTaxi_Clean` as a table in the Silver layer, we create a new dataframe, namely nyc_tlc_df_clean by reading the NYCTaxiClean table\n","\n","<font size=\"2\" color=\"red\" face=\"sans-serif\" bold> \n","\n","<b> <i> <u>In case you encounter any errors indicating that the table or the lakehouse cannot be located, please try refreshing your BronzeLakehouse and verifying the tablename once more\n","</font>\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d94ea83c-8394-42e9-9c4e-8659f398880d"},{"cell_type":"code","source":["# enrich taxi data with holiday data\n","You will be able to read data from different tables from different lakehouses from the same workspace by creating a dataframe.\n","\n","nyc_tlc_df_clean = spark.read.table(\"SilverLakehouse.NYCTaxi_Clean\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-09-20T12:18:18.2785449Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"9fecf23f-6bf2-4a7d-b2b2-0e0a47c2c7a9"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":17,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a0661f7b-cbd0-4ce5-9f06-73f8dc9fd0da"},{"cell_type":"markdown","source":["## Adding Transformations\n","Next, join the holiday data with the taxi data by performing a left-join. This will preserve all records from taxi data, but add in holiday data where it exists for the corresponding datetime and country_code, which in this case is always \"US\". Preview the data to verify that they were merged correctly.\n","\n","Basically during this step we perform the join between two datasets."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9c4b01d2-3d2d-43db-9d05-4d207aa12860"},{"cell_type":"code","source":["nyc_taxi_holiday_df = nyc_tlc_df_clean.join(hol_df_clean, on = ['datetime', 'country_code'] , how = 'left')\n","\n","nyc_taxi_holiday_df.show(5)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"98ecf1fe-53a7-4ce6-88b5-aa55965ec005"},{"cell_type":"markdown","source":["## Writing a Delta Table with Transformed Data\n","Before proceeding, it's a good idea to check if the table already exists. Right click on the Tables and hit refresh. If it doesn't, you'll need to uncomment the command below and execute the cell to create it."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d48bd5f6-c24f-45e8-ae1f-f2ee8cba9b9a"},{"cell_type":"code","source":["%%sql\n","-- DROP TABLE IF EXISTS SilverLakehouse.NYCTaxi_Holiday"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"}},"id":"8b5bfe07-fbe6-459b-8a10-13f7bd5b38d0"},{"cell_type":"markdown","source":["We're now ready to store the nyc_taxi_holiday_df as a table named NYCTaxi_Holiday on the Silver layer. By specifying the ABFSS path, we'll be able to easily create dataFrames from this table and use them for any future data transformations."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fe2e046d-31ca-418f-bde0-5a787e378e35"},{"cell_type":"code","source":["# Save the clean file as a table in the Silver Layer\n","\n","table_name = 'NYCTaxi_Holiday'\n","\n","nyc_taxi_holiday_df.write \\\n","    .mode(\"overwrite\") \\\n","    .format(\"delta\") \\\n","    .save(\"Tables/\" + table_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9fbe9f83-5d5d-49c0-89cd-900da34b83d6"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"microsoft":{"host":{"synapse_widget":{"token":"e92913df-b05a-4fb7-be5c-4b2164aa68cc","state":{}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}